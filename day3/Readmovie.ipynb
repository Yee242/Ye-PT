{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a0a661e-8b51-4d3e-9b3f-fb0b7b23e210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: requests in c:\\users\\86150\\appdata\\roaming\\python\\python311\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda2024\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda2024\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda2024\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda2024\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f527acf-efa1-4c05-8981-d697bcbdb227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始爬取豆瓣电影排行榜数据...\n",
      "成功获取 10 条电影数据\n",
      "1. 肖申克的救赎 - 评分: 9.7 - 评语: 暂无评价\n",
      "2. 霸王别姬 - 评分: 9.6 - 评语: 暂无评价\n",
      "3. 泰坦尼克号 - 评分: 9.5 - 评语: 暂无评价\n",
      "4. 阿甘正传 - 评分: 9.5 - 评语: 暂无评价\n",
      "5. 千与千寻 - 评分: 9.4 - 评语: 暂无评价\n",
      "6. 美丽人生 - 评分: 9.5 - 评语: 暂无评价\n",
      "7. 这个杀手不太冷 - 评分: 9.4 - 评语: 暂无评价\n",
      "8. 星际穿越 - 评分: 9.4 - 评语: 暂无评价\n",
      "9. 盗梦空间 - 评分: 9.4 - 评语: 暂无评价\n",
      "10. 楚门的世界 - 评分: 9.4 - 评语: 暂无评价\n",
      "数据已保存到 豆瓣电影Top10.xlsx\n",
      "爬取完成\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "def get_movie_info(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        response.encoding = response.apparent_encoding\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"请求出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_movie_data(html_content):\n",
    "    if not html_content:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    movie_list = []\n",
    "    \n",
    "    for item in soup.select('div.item')[:10]:\n",
    "        try:\n",
    "            rank = item.select_one('div.pic em').text.strip()\n",
    "            title = item.select_one('span.title').text.strip()\n",
    "            rating = item.select_one('span.rating_num').text.strip()\n",
    "            quote = item.select_one('span.inq')\n",
    "            quote = quote.text.strip() if quote else \"暂无评价\"\n",
    "            \n",
    "            movie_list.append({\n",
    "                '排名': rank,\n",
    "                '片名': title,\n",
    "                '评分': rating,\n",
    "                '评语': quote\n",
    "            })\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    \n",
    "    return movie_list\n",
    "\n",
    "def save_to_excel(data, filename='豆瓣电影Top10.xlsx'):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(filename, index=False, engine='openpyxl')\n",
    "    print(f\"数据已保存到 {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://movie.douban.com/top250'\n",
    "    print(\"开始爬取豆瓣电影排行榜数据...\")\n",
    "    \n",
    "    html_content = get_movie_info(url)\n",
    "    movie_data = parse_movie_data(html_content)\n",
    "    \n",
    "    if movie_data:\n",
    "        print(f\"成功获取 {len(movie_data)} 条电影数据\")\n",
    "        for movie in movie_data:\n",
    "            print(f\"{movie['排名']}. {movie['片名']} - 评分: {movie['评分']} - 评语: {movie['评语']}\")\n",
    "        \n",
    "        save_to_excel(movie_data)\n",
    "    else:\n",
    "        print(\"未能获取到电影数据\")\n",
    "    \n",
    "    print(\"爬取完成\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61955af2-5542-49ad-8b53-d1b33dbc0dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: lxml in d:\\anaconda2024\\lib\\site-packages (4.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "959c0af8-b687-4de2-b971-517151714300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 20 张图片\n",
      "正在下载第 1/20 张图片: http://pic.netbian.com/uploads/allimg/250627/202545-175102714571ed.jpg\n",
      "图片已保存至: d:/images\\image_0.jpg\n",
      "正在下载第 2/20 张图片: http://pic.netbian.com/uploads/allimg/250626/231909-17509511496e7b.jpg\n",
      "图片已保存至: d:/images\\image_1.jpg\n",
      "正在下载第 3/20 张图片: http://pic.netbian.com/uploads/allimg/250627/172845-1751016525523a.jpg\n",
      "图片已保存至: d:/images\\image_2.jpg\n",
      "正在下载第 4/20 张图片: http://pic.netbian.com/uploads/allimg/250627/170634-17510151946096.jpg\n",
      "图片已保存至: d:/images\\image_3.jpg\n",
      "正在下载第 5/20 张图片: http://pic.netbian.com/uploads/allimg/250627/105030-17509926303a62.jpg\n",
      "图片已保存至: d:/images\\image_4.jpg\n",
      "正在下载第 6/20 张图片: http://pic.netbian.com/uploads/allimg/250626/110455-1750907095c440.jpg\n",
      "图片已保存至: d:/images\\image_5.jpg\n",
      "正在下载第 7/20 张图片: http://pic.netbian.com/uploads/allimg/250625/233227-17508655473d8e.jpg\n",
      "图片已保存至: d:/images\\image_6.jpg\n",
      "正在下载第 8/20 张图片: http://pic.netbian.com/uploads/allimg/250614/201508-17499033082071.jpg\n",
      "图片已保存至: d:/images\\image_7.jpg\n",
      "正在下载第 9/20 张图片: http://pic.netbian.com/uploads/allimg/250627/165122-175101428227bb.jpg\n",
      "图片已保存至: d:/images\\image_8.jpg\n",
      "正在下载第 10/20 张图片: http://pic.netbian.com/uploads/allimg/250627/150752-17510080728d23.jpg\n",
      "图片已保存至: d:/images\\image_9.jpg\n",
      "正在下载第 11/20 张图片: http://pic.netbian.com/uploads/allimg/250627/150454-1751007894fdbb.jpg\n",
      "图片已保存至: d:/images\\image_10.jpg\n",
      "正在下载第 12/20 张图片: http://pic.netbian.com/uploads/allimg/250626/233023-1750951823a59e.jpg\n",
      "图片已保存至: d:/images\\image_11.jpg\n",
      "正在下载第 13/20 张图片: http://pic.netbian.com/uploads/allimg/250626/230830-1750950510bb26.jpg\n",
      "图片已保存至: d:/images\\image_12.jpg\n",
      "正在下载第 14/20 张图片: http://pic.netbian.com/uploads/allimg/250626/225348-17509496282fcb.jpg\n",
      "图片已保存至: d:/images\\image_13.jpg\n",
      "正在下载第 15/20 张图片: http://pic.netbian.com/uploads/allimg/250626/123608-1750912568ea1c.jpg\n",
      "图片已保存至: d:/images\\image_14.jpg\n",
      "正在下载第 16/20 张图片: http://pic.netbian.com/uploads/allimg/250625/231026-17508642261599.jpg\n",
      "图片已保存至: d:/images\\image_15.jpg\n",
      "正在下载第 17/20 张图片: http://pic.netbian.com/uploads/allimg/250625/230348-17508638283ee0.jpg\n",
      "图片已保存至: d:/images\\image_16.jpg\n",
      "正在下载第 18/20 张图片: http://pic.netbian.com/uploads/allimg/250623/212940-17506853800e79.jpg\n",
      "图片已保存至: d:/images\\image_17.jpg\n",
      "正在下载第 19/20 张图片: http://pic.netbian.com/uploads/allimg/250622/214542-175059994245b0.jpg\n",
      "图片已保存至: d:/images\\image_18.jpg\n",
      "正在下载第 20/20 张图片: http://pic.netbian.com/uploads/allimg/250622/213349-1750599229d2cc.jpg\n",
      "图片已保存至: d:/images\\image_19.jpg\n",
      "全部下载完成，共下载 20 张图片\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import os\n",
    "import time\n",
    "\n",
    "def download_images():\n",
    "    # 设置请求头，模拟浏览器访问\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    base_url = \"http://pic.netbian.com/\"\n",
    "    \n",
    "    try:\n",
    "        # 发送请求获取网页内容\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.encoding = \"gbk\"\n",
    "        html_content = response.text\n",
    "        \n",
    "        # 解析HTML内容\n",
    "        tree = etree.HTML(html_content)\n",
    "        \n",
    "        # 提取图片URL列表\n",
    "        img_urls = tree.xpath(\"//ul[@class='clearfix']/li/a/span/img/@src\")\n",
    "        \n",
    "        if not img_urls:\n",
    "            print(\"未找到图片URL\")\n",
    "            return\n",
    "        \n",
    "        print(f\"找到 {len(img_urls)} 张图片\")\n",
    "        \n",
    "        # 创建保存目录\n",
    "        save_dir = \"d:/images\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # 下载每张图片\n",
    "        for i, img_path in enumerate(img_urls):\n",
    "            # 处理相对URL\n",
    "            if img_path.startswith('/'):\n",
    "                img_url = base_url + img_path[1:]  # 移除开头的斜杠\n",
    "            else:\n",
    "                img_url = base_url + img_path\n",
    "            \n",
    "            try:\n",
    "                # 下载图片\n",
    "                print(f\"正在下载第 {i+1}/{len(img_urls)} 张图片: {img_url}\")\n",
    "                img_response = requests.get(img_url, headers=headers, timeout=10)\n",
    "                img_response.raise_for_status()\n",
    "                \n",
    "                # 确定文件扩展名\n",
    "                file_ext = os.path.splitext(img_url)[1] or '.jpg'\n",
    "                \n",
    "                # 保存图片\n",
    "                save_path = os.path.join(save_dir, f\"image_{i}{file_ext}\")\n",
    "                with open(save_path, 'wb') as f:\n",
    "                    f.write(img_response.content)\n",
    "                \n",
    "                print(f\"图片已保存至: {save_path}\")\n",
    "                \n",
    "                # 添加延时，避免频繁请求\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"下载图片失败: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"全部下载完成，共下载 {len(img_urls)} 张图片\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"发生错误: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_images()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825b620-4ae3-435d-8aaa-40dede592a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
